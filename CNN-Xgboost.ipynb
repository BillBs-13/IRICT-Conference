{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNySTi0+f7XHv3RXfN4XKjk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BillBs-13/IRICT-Conference/blob/main/Attention_Enhanced_CNN%E2%80%93ResNet_with_XGBoost_Ensem_ble_and_DAMCE_Loss_for_Intrusion_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NSL-KDD"
      ],
      "metadata": {
        "id": "y82hWx5ynS8L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3MzVjNkmPfp"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# NSL-KDD (kdd_train.csv / kdd_test.csv)\n",
        "# =========================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.metrics import (classification_report, accuracy_score, precision_score,\n",
        "                             recall_score, f1_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Dense, Dropout, BatchNormalization, Input, Conv1D,\n",
        "                                     MaxPooling1D, Flatten, Add, Layer, Multiply)\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --------------------\n",
        "# CONFIG\n",
        "# --------------------\n",
        "TRAIN_PATH   = \"/content/kdd_train.csv\"\n",
        "TEST_PATH    = \"/content/kdd_test.csv\"\n",
        "RANDOM_STATE = 42\n",
        "TOP_K        = 30\n",
        "\n",
        "LOSS_CHOICE  = \"damce\"   # \"cce\",\"bce\",\"mse\",\"mae_ce\",\"mse_ce\",\"focal\",\"dice\",\"tversky\",\"poly1\",\"damce\"\n",
        "BATCH_SIZE   = 1024\n",
        "EPOCHS       = 100\n",
        "VAL_PATIENCE = 12\n",
        "\n",
        "ENSEMBLE_W_NN  = 0.7\n",
        "ENSEMBLE_W_XGB = 0.3\n",
        "\n",
        "# --------------------\n",
        "# Losses\n",
        "# --------------------\n",
        "def loss_cce():  return tf.keras.losses.CategoricalCrossentropy()\n",
        "def loss_bce():  return tf.keras.losses.BinaryCrossentropy()\n",
        "def loss_mse():  return tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "def mae_ce_loss(alpha=0.5):\n",
        "    mae = tf.keras.losses.MeanAbsoluteError()\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    def fn(y_true, y_pred): return alpha*mae(y_true,y_pred) + (1.0-alpha)*bce(y_true,y_pred)\n",
        "    return fn\n",
        "\n",
        "def mse_ce_loss(alpha=0.5):\n",
        "    mse = tf.keras.losses.MeanSquaredError()\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    def fn(y_true, y_pred): return alpha*mse(y_true,y_pred) + (1.0-alpha)*bce(y_true,y_pred)\n",
        "    return fn\n",
        "\n",
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    def fn(y_true,y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, eps, 1.-eps)\n",
        "        ce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "        p_t = y_true*y_pred + (1-y_true)*(1-y_pred)\n",
        "        return tf.reduce_mean(alpha * tf.pow(1.-p_t, gamma) * ce)\n",
        "    return fn\n",
        "\n",
        "def dice_loss(eps=1e-6):\n",
        "    def fn(y_true,y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        inter = tf.reduce_sum(y_true*y_pred)\n",
        "        union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
        "        return 1. - (2.*inter + eps) / (union + eps)\n",
        "    return fn\n",
        "\n",
        "def tversky_loss(alpha=0.7,beta=0.3,eps=1e-6):\n",
        "    def fn(y_true,y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        TP = tf.reduce_sum(y_true*y_pred); FP = tf.reduce_sum((1-y_true)*y_pred); FN = tf.reduce_sum(y_true*(1-y_pred))\n",
        "        return 1. - (TP+eps)/(TP+alpha*FN+beta*FP+eps)\n",
        "    return fn\n",
        "\n",
        "def poly1_bce(epsilon=1.0):\n",
        "    def fn(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, eps, 1.-eps)\n",
        "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "        p_t = y_true*y_pred + (1.0 - y_true)*(1.0 - y_pred)\n",
        "        return tf.reduce_mean(bce + epsilon * (1.0 - p_t))\n",
        "    return fn\n",
        "\n",
        "def damce_loss(alpha=0.9, gamma=2.0):\n",
        "    \"\"\"DAMCE = alpha * w * CE + (1-alpha) * (1-w) * MSE, w=(1-p_t)^gamma\"\"\"\n",
        "    def fn(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, eps, 1.-eps)\n",
        "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)      # (B,)\n",
        "        mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)      # (B,)\n",
        "        p_t = y_true*y_pred + (1.0 - y_true)*(1.0 - y_pred)            # (B,2)\n",
        "        p_t = tf.reduce_mean(p_t, axis=-1)                              # (B,)\n",
        "        w = tf.pow(1.0 - p_t, gamma)\n",
        "        return tf.reduce_mean(alpha*w*bce + (1.0-alpha)*(1.0-w)*mse)\n",
        "    return fn\n",
        "\n",
        "def get_loss(name):\n",
        "    n = name.lower()\n",
        "    return {\n",
        "        \"cce\":    loss_cce(),\n",
        "        \"bce\":    loss_bce(),\n",
        "        \"mse\":    loss_mse(),\n",
        "        \"mae_ce\": mae_ce_loss(alpha=0.5),\n",
        "        \"mse_ce\": mse_ce_loss(alpha=0.5),\n",
        "        \"focal\":  focal_loss(),\n",
        "        \"dice\":   dice_loss(),\n",
        "        \"tversky\": tversky_loss(),\n",
        "        \"poly1\":  poly1_bce(epsilon=1.0),\n",
        "        \"damce\":  damce_loss(alpha=0.9, gamma=2.0),\n",
        "    }.get(n, loss_bce())\n",
        "\n",
        "# --------------------\n",
        "# Model\n",
        "# --------------------\n",
        "class Attention(Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.dense = Dense(units, activation='tanh')\n",
        "        self.score = Dense(1, activation='sigmoid')\n",
        "    def call(self, inputs):\n",
        "        scores = self.score(self.dense(inputs))\n",
        "        return Multiply()([inputs, scores])\n",
        "\n",
        "def residual_block(x, units, dropout_rate=0.3):\n",
        "    sc = x\n",
        "    x = Dense(units, activation='relu')(x); x = BatchNormalization()(x); x = Dropout(dropout_rate)(x)\n",
        "    x = Dense(units, activation=None)(x);   x = BatchNormalization()(x)\n",
        "    if sc.shape[-1] != units: sc = Dense(units, activation=None)(sc)\n",
        "    x = Add()([sc, x]); return tf.keras.activations.relu(x)\n",
        "\n",
        "def build_model(input_shape, num_classes, loss_choice):\n",
        "    use_softmax = loss_choice.lower() == \"cce\"\n",
        "    final_activation = \"softmax\" if use_softmax else \"sigmoid\"\n",
        "    inp = Input(shape=input_shape)\n",
        "    x = Conv1D(filters=32, kernel_size=3, activation='relu')(inp)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Flatten()(x)\n",
        "    cnn_features = Dense(64, activation='relu')(x)\n",
        "    attention_output = Attention(64)(cnn_features)\n",
        "    x = residual_block(attention_output, 64)\n",
        "    x = residual_block(x, 64)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    out = Dense(num_classes, activation=final_activation, dtype='float32')(x)\n",
        "    return Model(inputs=inp, outputs=out)\n",
        "\n",
        "# --------------------\n",
        "# Load NSL-KDD\n",
        "# --------------------\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "test_df  = pd.read_csv(TEST_PATH)\n",
        "\n",
        "label_col = \"labels\"\n",
        "\n",
        "# Binary mapping: normal -> 0, attack -> 1\n",
        "def make_binary(y):\n",
        "    if y.dtype == object:\n",
        "        return (y.str.lower() != \"normal\").astype(int).values\n",
        "    return (y.astype(str).str.lower() != \"normal\").astype(int).values\n",
        "\n",
        "y_train_raw = make_binary(train_df[label_col])\n",
        "y_test_raw  = make_binary(test_df[label_col])\n",
        "\n",
        "# Feature dataframe (drop label)\n",
        "X_train_df = train_df.drop(columns=[label_col], errors=\"ignore\")\n",
        "X_test_df  = test_df.drop(columns=[label_col],  errors=\"ignore\")\n",
        "\n",
        "# Categorical columns in NSL-KDD\n",
        "cat_cols = [\"protocol_type\", \"service\", \"flag\"]\n",
        "for c in cat_cols:\n",
        "    if c not in X_train_df.columns:\n",
        "        raise ValueError(f\"Expected categorical column '{c}' not found in data.\")\n",
        "\n",
        "# Encode categoricals jointly (train+test) to keep vocab consistent\n",
        "full = pd.concat([X_train_df, X_test_df], axis=0, ignore_index=True)\n",
        "for c in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    full[c] = le.fit_transform(full[c].astype(str))\n",
        "X_train_df = full.iloc[:len(X_train_df)].copy()\n",
        "X_test_df  = full.iloc[len(X_train_df):].copy()\n",
        "\n",
        "# --------------------\n",
        "# MI feature selection on TRAIN ONLY\n",
        "# --------------------\n",
        "mi_scores = mutual_info_classif(X_train_df.values, y_train_raw, discrete_features='auto',\n",
        "                                random_state=RANDOM_STATE)\n",
        "mi_ranking = pd.Series(mi_scores, index=X_train_df.columns).sort_values(ascending=False)\n",
        "top_features = mi_ranking.head(TOP_K).index.tolist()\n",
        "print(\"Top-{} MI features:\".format(TOP_K), top_features)\n",
        "\n",
        "X_train_raw = X_train_df[top_features].values\n",
        "X_test_raw  = X_test_df[top_features].values\n",
        "\n",
        "# --------------------\n",
        "# SMOTE, scale, reshape\n",
        "# --------------------\n",
        "sm = SMOTE(random_state=RANDOM_STATE)\n",
        "X_train_bal, y_train_bal = sm.fit_resample(X_train_raw, y_train_raw)\n",
        "X_train_bal, y_train_bal = shuffle(X_train_bal, y_train_bal, random_state=RANDOM_STATE)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train_bal)\n",
        "X_test  = scaler.transform(X_test_raw)\n",
        "\n",
        "y_train = to_categorical(y_train_bal, num_classes=2)\n",
        "y_test  = to_categorical(y_test_raw,  num_classes=2)\n",
        "num_classes = 2\n",
        "\n",
        "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_cnn  = X_test.reshape((X_test.shape[0],  X_test.shape[1],  1))\n",
        "\n",
        "# --------------------\n",
        "# Train NN\n",
        "# --------------------\n",
        "tf.keras.backend.clear_session()\n",
        "model = build_model((X_train.shape[1], 1), num_classes, LOSS_CHOICE)\n",
        "loss_fn = get_loss(LOSS_CHOICE)\n",
        "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=1e-3, decay_steps=2000)\n",
        "optimizer   = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=VAL_PATIENCE, restore_best_weights=True)]\n",
        "history = model.fit(\n",
        "    X_train_cnn, y_train,\n",
        "    epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "    validation_split=0.1, callbacks=callbacks, verbose=2\n",
        ")\n",
        "\n",
        "# --------------------\n",
        "# Train XGBoost\n",
        "# --------------------\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    use_label_encoder=False, eval_metric='logloss', n_jobs=-1,\n",
        "    tree_method=\"hist\", max_depth=6, n_estimators=500, learning_rate=0.08,\n",
        "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0\n",
        ")\n",
        "xgb_model.fit(X_train, y_train_bal)\n",
        "\n",
        "# --------------------\n",
        "# Inference + metrics\n",
        "# --------------------\n",
        "preds_nn  = model.predict(X_test_cnn, verbose=0)\n",
        "preds_xgb = xgb_model.predict_proba(X_test)\n",
        "final_preds = ENSEMBLE_W_NN * preds_nn + ENSEMBLE_W_XGB * preds_xgb\n",
        "\n",
        "final_class = np.argmax(final_preds, axis=1)\n",
        "true_class  = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report (Weighted Voting Ensemble):\")\n",
        "print(classification_report(true_class, final_class, digits=4))\n",
        "\n",
        "acc       = accuracy_score(true_class, final_class)\n",
        "precision = precision_score(true_class, final_class, average='weighted', zero_division=0)\n",
        "recall    = recall_score(true_class, final_class, average='weighted', zero_division=0)\n",
        "f1        = f1_score(true_class, final_class, average='weighted', zero_division=0)\n",
        "fpr, tpr, _ = roc_curve(y_test.ravel(), final_preds.ravel())\n",
        "roc_auc   = auc(fpr, tpr)\n",
        "\n",
        "print(f\"\\nAccuracy:  {acc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(f\"AUC:       {roc_auc:.4f}\")\n",
        "\n",
        "plt.figure()\n",
        "ConfusionMatrixDisplay(confusion_matrix(true_class, final_class)).plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\"); plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy over Epochs\"); plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\"); plt.title(\"Loss over Epochs\"); plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNSW-NB15"
      ],
      "metadata": {
        "id": "4TCWIslenncp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.metrics import (classification_report, accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Dense, Dropout, BatchNormalization, Input, Conv1D,\n",
        "                                     MaxPooling1D, Flatten, Add, Layer, Multiply)\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# =========================\n",
        "LOSS_CHOICE = \"damce\"  # one of: \"cce\",\"bce\",\"mse\",\"mae_ce\",\"mse_ce\",\"focal\",\"dice\",\"tversky\",\"poly1\",\"damce\"\n",
        "\n",
        "# === Custom Layers ===\n",
        "class Attention(Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.dense = Dense(units, activation='tanh')\n",
        "        self.score = Dense(1, activation='sigmoid')\n",
        "    def call(self, inputs):\n",
        "        scores = self.score(self.dense(inputs))\n",
        "        return Multiply()([inputs, scores])\n",
        "\n",
        "def residual_block(x, units, dropout_rate=0.3):\n",
        "    shortcut = x\n",
        "    x = Dense(units, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = Dense(units, activation=None)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if shortcut.shape[-1] != units:\n",
        "        shortcut = Dense(units, activation=None)(shortcut)\n",
        "    x = Add()([shortcut, x])\n",
        "    x = tf.keras.activations.relu(x)\n",
        "    return x\n",
        "\n",
        "# =========================\n",
        "# Loss functions (full set)\n",
        "# =========================\n",
        "def loss_cce():\n",
        "    return tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "def loss_bce():\n",
        "    return tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def loss_mse():\n",
        "    return tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "def mae_ce_loss(alpha=0.5):\n",
        "    mae = tf.keras.losses.MeanAbsoluteError()\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    def fn(y_true, y_pred):\n",
        "        return alpha*mae(y_true,y_pred) + (1.0-alpha)*bce(y_true,y_pred)\n",
        "    return fn\n",
        "\n",
        "def mse_ce_loss(alpha=0.5):\n",
        "    mse = tf.keras.losses.MeanSquaredError()\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    def fn(y_true, y_pred):\n",
        "        return alpha*mse(y_true,y_pred) + (1.0-alpha)*bce(y_true,y_pred)\n",
        "    return fn\n",
        "\n",
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    def fn(y_true,y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, eps, 1.-eps)\n",
        "        ce = tf.keras.losses.binary_crossentropy(y_true, y_pred)   # per-sample BCE\n",
        "        p_t = y_true*y_pred + (1-y_true)*(1-y_pred)\n",
        "        return tf.reduce_mean(alpha * tf.pow(1. - p_t, gamma) * ce)\n",
        "    return fn\n",
        "\n",
        "def dice_loss(eps=1e-6):\n",
        "    def fn(y_true,y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        inter = tf.reduce_sum(y_true*y_pred)\n",
        "        union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
        "        return 1. - (2.*inter + eps) / (union + eps)\n",
        "    return fn\n",
        "\n",
        "def tversky_loss(alpha=0.7, beta=0.3, eps=1e-6):\n",
        "    def fn(y_true,y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        TP = tf.reduce_sum(y_true*y_pred)\n",
        "        FP = tf.reduce_sum((1-y_true)*y_pred)\n",
        "        FN = tf.reduce_sum(y_true*(1-y_pred))\n",
        "        return 1. - (TP + eps) / (TP + alpha*FN + beta*FP + eps)\n",
        "    return fn\n",
        "\n",
        "def poly1_bce(epsilon=1.0):\n",
        "    def fn(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, eps, 1.-eps)\n",
        "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "        p_t = y_true*y_pred + (1.0 - y_true)*(1.0 - y_pred)\n",
        "        return tf.reduce_mean(bce + epsilon * (1.0 - p_t))\n",
        "    return fn\n",
        "\n",
        "def damce_loss(alpha=0.1, gamma=2.0):\n",
        "    \"\"\"\n",
        "    Difficulty-Aware MSE+CE (DAMCE):\n",
        "    loss = alpha * w * CE + (1-alpha) * (1-w) * MSE\n",
        "    with w = (1 - p_t)^gamma, p_t = y*p + (1-y)*(1-p)\n",
        "    \"\"\"\n",
        "    def fn(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, eps, 1.-eps)\n",
        "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)        # (B,)\n",
        "        mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)        # (B,)\n",
        "        p_t = y_true*y_pred + (1.0 - y_true)*(1.0 - y_pred)              # (B, C)\n",
        "        p_t = tf.reduce_mean(p_t, axis=-1)                                # (B,)\n",
        "        w = tf.pow(1.0 - p_t, gamma)                                      # (B,)\n",
        "        return tf.reduce_mean(alpha*w*bce + (1.0-alpha)*(1.0-w)*mse)\n",
        "    return fn\n",
        "\n",
        "def get_loss(name):\n",
        "    n = name.lower()\n",
        "    table = {\n",
        "        \"cce\":    loss_cce(),\n",
        "        \"bce\":    loss_bce(),\n",
        "        \"mse\":    loss_mse(),\n",
        "        \"mae_ce\": mae_ce_loss(alpha=0.5),\n",
        "        \"mse_ce\": mse_ce_loss(alpha=0.5),\n",
        "        \"focal\":  focal_loss(),\n",
        "        \"dice\":   dice_loss(),\n",
        "        \"tversky\": tversky_loss(),\n",
        "        \"poly1\":  poly1_bce(epsilon=1.0),\n",
        "        \"damce\":  damce_loss(alpha=0.9, gamma=2.0),\n",
        "    }\n",
        "    return table.get(n, loss_bce())\n",
        "\n",
        "def build_model(input_shape, num_classes, loss_choice=LOSS_CHOICE):\n",
        "\n",
        "    use_softmax = loss_choice.lower() == \"cce\"\n",
        "    final_activation = \"softmax\" if use_softmax else \"sigmoid\"\n",
        "\n",
        "    inp = Input(shape=input_shape)\n",
        "    x = Conv1D(filters=32, kernel_size=3, activation='relu')(inp)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Flatten()(x)\n",
        "    cnn_features = Dense(64, activation='relu')(x)\n",
        "    attention_output = Attention(64)(cnn_features)\n",
        "    x = residual_block(attention_output, 64)\n",
        "    x = residual_block(x, 64)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    out = Dense(num_classes, activation=final_activation, dtype='float32')(x)\n",
        "    return Model(inputs=inp, outputs=out)\n",
        "\n",
        "# === Load and preprocess data ===\n",
        "train_df = pd.read_csv('/content/UNSW_NB15_training-set.csv')\n",
        "test_df  = pd.read_csv('/content/UNSW_NB15_testing-set.csv')\n",
        "train_df = train_df.drop(columns=['id', 'attack_cat'])\n",
        "test_df  = test_df.drop(columns=['id', 'attack_cat'])\n",
        "\n",
        "# === Encode categorical columns ===\n",
        "categorical_cols = ['proto', 'service', 'state']\n",
        "le = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    combined = pd.concat([train_df[col], test_df[col]], ignore_index=True)\n",
        "    le.fit(combined)\n",
        "    train_df[col] = le.transform(train_df[col])\n",
        "    test_df[col]  = le.transform(test_df[col])\n",
        "\n",
        "# === Prepare raw features and labels ===\n",
        "X_train_raw = train_df.drop(columns=['label']).values\n",
        "y_train_raw = train_df['label'].values\n",
        "X_test_raw  = test_df.drop(columns=['label']).values\n",
        "y_test_raw  = test_df['label'].values\n",
        "\n",
        "# === Feature Selection using Mutual Information ===\n",
        "feature_names = train_df.drop(columns=['label']).columns\n",
        "mi_scores  = mutual_info_classif(X_train_raw, y_train_raw, discrete_features='auto', random_state=42)\n",
        "mi_ranking = pd.Series(mi_scores, index=feature_names).sort_values(ascending=False)\n",
        "top_features = mi_ranking.head(30).index.tolist()\n",
        "print(\"Top 30 selected features:\\n\", top_features)\n",
        "\n",
        "# === Reduce data to selected features ===\n",
        "X_train_raw = train_df[top_features].values\n",
        "X_test_raw  = test_df[top_features].values\n",
        "\n",
        "# === Apply SMOTE and shuffle ===\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_bal, y_train_bal = sm.fit_resample(X_train_raw, y_train_raw)\n",
        "X_train_bal, y_train_bal = shuffle(X_train_bal, y_train_bal, random_state=42)\n",
        "\n",
        "# === Scaling ===\n",
        "scaler  = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train_bal)\n",
        "X_test  = scaler.transform(X_test_raw)\n",
        "\n",
        "# === One-hot encode labels ===\n",
        "y_train = to_categorical(y_train_bal)\n",
        "y_test  = to_categorical(y_test_raw)\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "# === Reshape for CNN ===\n",
        "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_cnn  = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# === Train CNN model ===\n",
        "tf.keras.backend.clear_session()\n",
        "model = build_model((X_train.shape[1], 1), num_classes, loss_choice=LOSS_CHOICE)\n",
        "loss_fn = get_loss(LOSS_CHOICE)\n",
        "\n",
        "# safer schedule\n",
        "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=1e-3, decay_steps=2000)\n",
        "optimizer   = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=12, restore_best_weights=True)]\n",
        "history = model.fit(X_train_cnn, y_train, epochs=100, batch_size=1024,\n",
        "                    validation_split=0.1, callbacks=callbacks, verbose=2)\n",
        "\n",
        "# === Train XGBoost ===\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', n_jobs=-1)\n",
        "xgb_model.fit(X_train, y_train_bal)\n",
        "\n",
        "# === Ensemble Inference (Weighted Voting) ===\n",
        "preds_nn  = model.predict(X_test_cnn, verbose=0)\n",
        "preds_xgb = xgb_model.predict_proba(X_test)\n",
        "final_preds = 0.7 * preds_nn + 0.3 * preds_xgb\n",
        "final_class = np.argmax(final_preds, axis=1)\n",
        "true_class  = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report (Weighted Voting Ensemble):\")\n",
        "print(classification_report(true_class, final_class))\n",
        "\n",
        "# === Metrics ===\n",
        "acc       = accuracy_score(true_class, final_class)\n",
        "precision = precision_score(true_class, final_class, average='weighted', zero_division=0)\n",
        "recall    = recall_score(true_class, final_class, average='weighted', zero_division=0)\n",
        "f1        = f1_score(true_class, final_class, average='weighted', zero_division=0)\n",
        "fpr, tpr, _ = roc_curve(y_test.ravel(), final_preds.ravel())\n",
        "roc_auc   = auc(fpr, tpr)\n",
        "\n",
        "print(f\"\\nAccuracy:  {acc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(f\"AUC:       {roc_auc:.4f}\")\n",
        "\n",
        "# === Confusion Matrix ===\n",
        "plt.figure()\n",
        "ConfusionMatrixDisplay(confusion_matrix(true_class, final_class)).plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "\n",
        "# === ROC Curve ===\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "\n",
        "# === Accuracy over Epochs ===\n",
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy over Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "# === Loss over Epochs ===\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss over Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q0MbYVCJnqJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CICIDS2017"
      ],
      "metadata": {
        "id": "OPWP6yeEoqGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# BLOCK 1: MI Feature Selection + Stats Cache\n",
        "# ==================================================\n",
        "import os, gc, math, json, numpy as np, pandas as pd\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from pandas.errors import ParserError\n",
        "\n",
        "# --------------------\n",
        "# CONFIG (static per dataset)\n",
        "# --------------------\n",
        "FILES = {\n",
        "    \"Mon\": \"/content/monday.csv\",\n",
        "    \"Tue\": \"/content/tuesday.csv\",\n",
        "    \"Wed\": \"/content/wednesday.csv\",\n",
        "    \"Thu\": \"/content/thursday.csv\",\n",
        "    \"Fri\": \"/content/friday.csv\",\n",
        "}\n",
        "CHUNKSIZE      = 250_000\n",
        "MI_CAP_PER_CLS = 150_000   # sample cap PER CLASS for MI\n",
        "TOP_K          = 30\n",
        "RANDOM_STATE   = 42\n",
        "\n",
        "DROP_COLS = [\"id\",\"Flow ID\",\"Src IP\",\"Dst IP\",\"Timestamp\",\"Label\",\"Attempted Category\"]\n",
        "MI_CACHE  = \"/content/mi_cache.json\"\n",
        "\n",
        "# --------------------\n",
        "# Helpers\n",
        "# --------------------\n",
        "def read_csv_smart(path, chunksize):\n",
        "    try:\n",
        "        it = pd.read_csv(path, chunksize=chunksize, low_memory=False,\n",
        "                         na_values=[\"NaN\",\"Infinity\",\"-Infinity\"])\n",
        "        yield next(it)\n",
        "        for c in it: yield c\n",
        "    except ParserError:\n",
        "        # If default separator fails, try semicolon\n",
        "        for c in pd.read_csv(path, chunksize=chunksize, low_memory=False,\n",
        "                             sep=';', na_values=[\"NaN\",\"Infinity\",\"-Infinity\"]):\n",
        "            yield c\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while reading {path}: {e}\")\n",
        "        raise\n",
        "\n",
        "def map_binary_labels(df):\n",
        "    if \"Label\" not in df.columns: return None\n",
        "    ls = df[\"Label\"].astype(str)\n",
        "    attempted = df[\"Attempted Category\"].fillna(-1).astype(\"int32\").ne(-1).values \\\n",
        "                if \"Attempted Category\" in df.columns else ls.str.contains(\" - Attempted\", regex=False).values\n",
        "    benign = ls.eq(\"BENIGN\").values\n",
        "    return np.where(benign | attempted, 0, 1)\n",
        "\n",
        "# Welford running mean/var (NaN-safe per column)\n",
        "class RunningMoments:\n",
        "    def __init__(self):\n",
        "        self.n = 0\n",
        "        self.mean = None\n",
        "        self.M2 = None\n",
        "    def update(self, X_np):\n",
        "        if self.mean is None:\n",
        "            self.mean = np.zeros(X_np.shape[1], dtype=np.float64)\n",
        "            self.M2   = np.zeros(X_np.shape[1], dtype=np.float64)\n",
        "        valid = ~np.isnan(X_np)\n",
        "        counts = valid.sum(axis=0).astype(np.int64)\n",
        "        for j in range(X_np.shape[1]):\n",
        "            cnt = counts[j]\n",
        "            if cnt == 0: continue\n",
        "            xj = X_np[valid[:, j], j]\n",
        "            batch_mean = float(xj.mean())\n",
        "            delta = batch_mean - self.mean[j]\n",
        "            tot_n = self.n + cnt\n",
        "            self.mean[j] += delta * (cnt / max(1, tot_n))\n",
        "            self.M2[j] += xj.var(ddof=0)*cnt + (delta**2) * (self.n * cnt / max(1, tot_n))\n",
        "            self.n += cnt\n",
        "    def finalize(self):\n",
        "        var = self.M2 / np.maximum(1, self.n)\n",
        "        std = np.sqrt(np.maximum(var, 1e-12))\n",
        "        return self.mean.astype(np.float32), std.astype(np.float32)\n",
        "\n",
        "# --------------------\n",
        "# PASS 1: schema + stats + MI\n",
        "# --------------------\n",
        "print(\"[PASS1] Streaming for schema, running stats, MI sample…\")\n",
        "keep_cols = None\n",
        "moments = RunningMoments()\n",
        "mi_parts0, mi_parts1 = [], []\n",
        "n0 = n1 = 0\n",
        "total_rows = 0\n",
        "global_counts = {0:0, 1:0}\n",
        "\n",
        "for _, path in FILES.items():\n",
        "    for chunk in read_csv_smart(path, CHUNKSIZE):\n",
        "        y = map_binary_labels(chunk)\n",
        "        if y is None: continue\n",
        "\n",
        "        total_rows += len(y)\n",
        "        global_counts[0] += int((y==0).sum())\n",
        "        global_counts[1] += int((y==1).sum())\n",
        "\n",
        "        X = chunk.drop(columns=[c for c in DROP_COLS if c in chunk.columns], errors=\"ignore\")\n",
        "        X = X.apply(pd.to_numeric, errors=\"coerce\").replace([np.inf,-np.inf], np.nan)\n",
        "\n",
        "        if keep_cols is None:\n",
        "            keep_cols = list(X.columns)\n",
        "        X = X.reindex(columns=keep_cols).astype(np.float32)\n",
        "\n",
        "        # update running moments\n",
        "        moments.update(X.values)\n",
        "\n",
        "        # bounded MI sample (stratified)\n",
        "        need0 = max(0, MI_CAP_PER_CLS - n0)\n",
        "        need1 = max(0, MI_CAP_PER_CLS - n1)\n",
        "        if need0 > 0:\n",
        "            X0 = X[y==0]\n",
        "            if len(X0) > 0:\n",
        "                take0 = X0.sample(n=min(need0, len(X0)), random_state=RANDOM_STATE)\n",
        "                mi_parts0.append(take0); n0 += len(take0)\n",
        "        if need1 > 0:\n",
        "            X1 = X[y==1]\n",
        "            if len(X1) > 0:\n",
        "                take1 = X1.sample(n=min(need1, len(X1)), random_state=RANDOM_STATE)\n",
        "                mi_parts1.append(take1); n1 += len(take1)\n",
        "\n",
        "        del chunk, X\n",
        "        gc.collect()\n",
        "\n",
        "print(f\"[PASS1] Rows={total_rows:,} | global class counts: {global_counts}\")\n",
        "col_means, col_stds = moments.finalize()\n",
        "\n",
        "# MI on sample\n",
        "if mi_parts0 or mi_parts1:\n",
        "    X_mi = pd.concat(mi_parts0 + mi_parts1, ignore_index=True)\n",
        "    y_mi = np.array([0]*n0 + [1]*n1, dtype=np.int8)\n",
        "    mi_means = pd.Series(col_means, index=keep_cols)\n",
        "    X_mi = X_mi.fillna(mi_means)\n",
        "    mi_scores = mutual_info_classif(X_mi.values, y_mi, discrete_features='auto', random_state=RANDOM_STATE)\n",
        "    top_features = pd.Series(mi_scores, index=keep_cols).sort_values(ascending=False).head(TOP_K).index.tolist()\n",
        "else:\n",
        "    top_features = keep_cols[:TOP_K]\n",
        "print(f\"[PASS1] Top-{TOP_K} features: {top_features[:10]} …\")\n",
        "\n",
        "# class weights / xgb spw\n",
        "neg, pos = global_counts[0], global_counts[1]\n",
        "class_weight = {0: 1.0, 1: (neg / max(1, pos))}\n",
        "xgb_spw = neg / max(1, pos)\n",
        "\n",
        "# save cache\n",
        "with open(MI_CACHE, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"keep_cols\": keep_cols,\n",
        "        \"top_features\": top_features,\n",
        "        \"col_means\": col_means.tolist(),\n",
        "        \"col_stds\": col_stds.tolist(),\n",
        "        \"class_weight\": class_weight,\n",
        "        \"xgb_spw\": float(xgb_spw),\n",
        "        \"total_rows\": int(total_rows)\n",
        "    }, f)\n",
        "print(f\"[PASS1] Cached MI & stats -> {MI_CACHE}\")"
      ],
      "metadata": {
        "id": "2t9VfCZsosaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# BLOCK 2 (CNS 5-day): Train + Evaluate (loads MI cache; fast experiments)\n",
        "# ==================================================\n",
        "import os, gc, math, json, numpy as np, pandas as pd, tensorflow as tf, xgboost as xgb\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Conv1D, MaxPooling1D, Flatten, Add, Layer, Multiply\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# --------------------\n",
        "# CONFIG\n",
        "# --------------------\n",
        "FILES = {\n",
        "    \"Mon\": \"/content/monday.csv\",\n",
        "    \"Tue\": \"/content/tuesday.csv\",\n",
        "    \"Wed\": \"/content/wednesday.csv\",\n",
        "    \"Thu\": \"/content/thursday.csv\",\n",
        "    \"Fri\": \"/content/friday.csv\",\n",
        "}\n",
        "CHUNKSIZE      = 250_000\n",
        "BATCH_SIZE_NN  = 1024\n",
        "EPOCHS         = 10\n",
        "LOSS_CHOICE    = \"damce\"        # \"bce\",\"cce\",\"mse\",\"focal\",\"dice\",\"tversky\",\"poly1\",\"bhfdl\"\n",
        "ENSEMBLE_W_NN  = 0.7\n",
        "ENSEMBLE_W_XGB = 0.3\n",
        "XGB_MAX_DEPTH  = 6\n",
        "XGB_ROUNDS_PER_BATCH = 30\n",
        "RANDOM_STATE   = 42\n",
        "\n",
        "DROP_COLS = [\"id\",\"Flow ID\",\"Src IP\",\"Dst IP\",\"Timestamp\",\"Label\",\"Attempted Category\"]\n",
        "MI_CACHE  = \"/content/mi_cache.json\"\n",
        "\n",
        "# --------------------\n",
        "# Load MI cache\n",
        "# --------------------\n",
        "with open(MI_CACHE, \"r\") as f:\n",
        "    _c = json.load(f)\n",
        "keep_cols    = _c[\"keep_cols\"]\n",
        "top_features = _c[\"top_features\"]\n",
        "col_means    = np.array(_c[\"col_means\"], dtype=np.float32)\n",
        "col_stds     = np.array(_c[\"col_stds\"],  dtype=np.float32)\n",
        "mean_dict    = {c: col_means[i] for i, c in enumerate(keep_cols)}\n",
        "std_dict     = {c: (col_stds[i] if col_stds[i] > 1e-12 else 1.0) for i, c in enumerate(keep_cols)}\n",
        "class_weight = {int(k): float(v) for k,v in _c[\"class_weight\"].items()}\n",
        "xgb_spw      = float(_c[\"xgb_spw\"])\n",
        "total_rows   = int(_c[\"total_rows\"])\n",
        "feat_dim     = len(top_features)\n",
        "print(f\"[CACHE] Loaded. feat_dim={feat_dim}, total_rows≈{total_rows:,}\")\n",
        "\n",
        "# --------------------\n",
        "# Losses\n",
        "# --------------------\n",
        "def loss_cce():  return tf.keras.losses.CategoricalCrossentropy()\n",
        "def loss_bce():  return tf.keras.losses.BinaryCrossentropy()\n",
        "def loss_mse():  return tf.keras.losses.MeanSquaredError()  # Brier-like; stable\n",
        "def damce_loss(alpha=0.5, gamma=2.0):\n",
        "    \"\"\"\n",
        "    Difficulty-Aware MSE+CE (DAMCE):\n",
        "    - CE weight w = (1 - p_t)^gamma  (harder → higher CE weight)\n",
        "    - MSE weight = 1 - w             (easier → higher MSE weight)\n",
        "    Final = alpha * w * CE + (1-alpha) * (1-w) * MSE\n",
        "    \"\"\"\n",
        "    def fn(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, eps, 1. - eps)\n",
        "\n",
        "\n",
        "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "\n",
        "        mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)  # shape (B,)\n",
        "\n",
        "        # true-class confidence p_t (average across classes for 1-hot -> same as true class prob)\n",
        "        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)    # (B, C)\n",
        "        p_t = tf.reduce_mean(p_t, axis=-1)                         # (B,)\n",
        "\n",
        "        # difficulty weight\n",
        "        w = tf.pow(1.0 - p_t, gamma)                               # (B,)\n",
        "\n",
        "        loss_vec = alpha * w * bce + (1.0 - alpha) * (1.0 - w) * mse\n",
        "        return tf.reduce_mean(loss_vec)\n",
        "    return fn\n",
        "\n",
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    def fn(y_true,y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, eps, 1. - eps)\n",
        "        ce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "        p_t = y_true*y_pred + (1-y_true)*(1-y_pred)\n",
        "        return tf.reduce_mean(alpha*tf.pow(1.-p_t, gamma)*ce)\n",
        "    return fn\n",
        "\n",
        "def dice_loss(eps=1e-6):\n",
        "    def fn(y_true,y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        inter = tf.reduce_sum(y_true*y_pred)\n",
        "        union = tf.reduce_sum(y_true)+tf.reduce_sum(y_pred)\n",
        "        return 1. - (2.*inter+eps)/(union+eps)\n",
        "    return fn\n",
        "\n",
        "def tversky_loss(alpha=0.8,beta=0.2,eps=1e-6):\n",
        "    def fn(y_true,y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        TP = tf.reduce_sum(y_true*y_pred); FP = tf.reduce_sum((1-y_true)*y_pred); FN = tf.reduce_sum(y_true*(1-y_pred))\n",
        "        return 1. - (TP+eps)/(TP+alpha*FN+beta*FP+eps)\n",
        "    return fn\n",
        "\n",
        "\n",
        "def poly1_bce(epsilon=1.0):\n",
        "    def fn(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, eps, 1. - eps)\n",
        "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "        p_t = y_true*y_pred + (1.0 - y_true)*(1.0 - y_pred)\n",
        "        poly = bce + epsilon * (1.0 - p_t)\n",
        "        return tf.reduce_mean(poly)\n",
        "    return fn\n",
        "def mae_ce_loss(alpha=0.5):\n",
        "    mae = tf.keras.losses.MeanAbsoluteError()\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    def fn(y_true, y_pred):\n",
        "        return alpha*mae(y_true,y_pred) + (1.0-alpha)*bce(y_true,y_pred)\n",
        "    return fn\n",
        "\n",
        "def mse_ce_loss(alpha=0.5):\n",
        "    mse = tf.keras.losses.MeanSquaredError()\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    def fn(y_true, y_pred):\n",
        "        return alpha*mse(y_true,y_pred) + (1.0-alpha)*bce(y_true,y_pred)\n",
        "    return fn\n",
        "\n",
        "\n",
        "def bhfdl(lambda_=0.5,gamma=2.0,alpha=0.25):\n",
        "    fl = focal_loss(gamma=gamma, alpha=alpha); dl = dice_loss()\n",
        "    def fn(y_true,y_pred): return lambda_*fl(y_true,y_pred) + (1.-lambda_)*dl(y_true,y_pred)\n",
        "    return fn\n",
        "\n",
        "def get_loss(name):\n",
        "    n = name.lower()\n",
        "    table = {\n",
        "        \"cce\":    loss_cce(),\n",
        "        \"bce\":    loss_bce(),\n",
        "        \"mse\":    loss_mse(),\n",
        "        \"focal\":  focal_loss(),\n",
        "        \"dice\":   dice_loss(),\n",
        "        \"tversky\": tversky_loss(),\n",
        "        \"poly1\":  poly1_bce(epsilon=1.0),\n",
        "        \"bhfdl\":  bhfdl(),\n",
        "        \"mae_ce\": mae_ce_loss(alpha=0.5),\n",
        "        \"mse_ce\": mse_ce_loss(alpha=0.5),\n",
        "        \"damce\":  damce_loss(alpha=0.5, gamma=2.0),  # <— NEW\n",
        "    }\n",
        "    return table.get(n, loss_bce())\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Model\n",
        "# --------------------\n",
        "class Attention(Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.dense = Dense(units, activation='tanh')\n",
        "        self.score = Dense(1, activation='sigmoid')\n",
        "    def call(self, inputs):\n",
        "        scores = self.score(self.dense(inputs))\n",
        "        return Multiply()([inputs, scores])\n",
        "\n",
        "def residual_block(x, units, dropout_rate=0.3):\n",
        "    sc = x\n",
        "    x = Dense(units, activation='relu')(x); x = BatchNormalization()(x); x = Dropout(dropout_rate)(x)\n",
        "    x = Dense(units, activation=None)(x);   x = BatchNormalization()(x)\n",
        "    if sc.shape[-1] != units: sc = Dense(units, activation=None)(sc)\n",
        "    x = Add()([sc, x]); return tf.keras.activations.relu(x)\n",
        "\n",
        "def build_model(input_shape, num_classes=2):\n",
        "    inp = Input(shape=input_shape)\n",
        "    x = Conv1D(filters=32, kernel_size=3, activation='relu')(inp)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Flatten()(x)\n",
        "    cnn_features = Dense(64, activation='relu')(x)\n",
        "    attention_output = Attention(64)(cnn_features)\n",
        "    x = residual_block(attention_output, 64)\n",
        "    x = residual_block(x, 64)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    out = Dense(num_classes, activation='sigmoid', dtype='float32')(x)\n",
        "    return Model(inputs=inp, outputs=out)\n",
        "\n",
        "# --------------------\n",
        "# Streaming readers\n",
        "# --------------------\n",
        "def read_csv_smart(path, chunksize):\n",
        "    try:\n",
        "        it = pd.read_csv(path, chunksize=chunksize, low_memory=False,\n",
        "                         na_values=[\"NaN\",\"Infinity\",\"-Infinity\"])\n",
        "        first = next(it)\n",
        "        if first.shape[1] == 1: raise ValueError(\"Bad sep, retry ;\")\n",
        "        yield first\n",
        "        for c in it: yield c\n",
        "    except Exception:\n",
        "        for c in pd.read_csv(path, chunksize=chunksize, low_memory=False, sep=';',\n",
        "                             na_values=[\"NaN\",\"Infinity\",\"-Infinity\"]):\n",
        "            yield c\n",
        "\n",
        "def map_binary_labels(df):\n",
        "    if \"Label\" not in df.columns: return None\n",
        "    ls = df[\"Label\"].astype(str)\n",
        "    attempted = df[\"Attempted Category\"].fillna(-1).astype(\"int32\").ne(-1).values \\\n",
        "                if \"Attempted Category\" in df.columns else ls.str.contains(\" - Attempted\", regex=False).values\n",
        "    benign = ls.eq(\"BENIGN\").values\n",
        "    return np.where(benign | attempted, 0, 1)\n",
        "\n",
        "def stream_batches(files_map, batch_size, selected_cols, means, stds, y_onehot=True, repeat_epochs=1):\n",
        "    cols = selected_cols\n",
        "    for _ in range(repeat_epochs):\n",
        "        for _, path in files_map.items():\n",
        "            X_buf, y_buf = [], []\n",
        "            for chunk in read_csv_smart(path, CHUNKSIZE):\n",
        "                y = map_binary_labels(chunk)\n",
        "                if y is None: continue\n",
        "                X = chunk.drop(columns=[c for c in DROP_COLS if c in chunk.columns], errors=\"ignore\")\n",
        "                X = X.reindex(columns=keep_cols).astype(np.float32)\n",
        "                X = X.fillna(pd.Series(means))\n",
        "                X = X[cols]\n",
        "                X = (X - pd.Series({c: means[c] for c in cols})) / pd.Series({c: stds[c] for c in cols})\n",
        "\n",
        "                Xv = X.values.astype(np.float32)\n",
        "                yv = y.astype(np.int8)\n",
        "                if y_onehot: yv = to_categorical(yv, num_classes=2).astype(np.float32)\n",
        "\n",
        "                X_buf.append(Xv); y_buf.append(yv)\n",
        "\n",
        "                while sum(len(b) for b in X_buf) >= batch_size:\n",
        "                    need = batch_size; xb_parts, yb_parts = [], []\n",
        "                    while need > 0 and X_buf:\n",
        "                        take = min(need, len(X_buf[0]))\n",
        "                        xb_parts.append(X_buf[0][:take]); yb_parts.append(y_buf[0][:take])\n",
        "                        X_buf[0] = X_buf[0][take:]; y_buf[0] = y_buf[0][take:]\n",
        "                        if len(X_buf[0]) == 0: X_buf.pop(0); y_buf.pop(0)\n",
        "                        need -= take\n",
        "                    Xb = np.vstack(xb_parts)\n",
        "                    yb = np.vstack(yb_parts) if y_onehot else np.concatenate(yb_parts).astype(np.int8)\n",
        "                    yield Xb, yb\n",
        "\n",
        "                del chunk, X, Xv, yv; gc.collect()\n",
        "\n",
        "            if X_buf:\n",
        "                Xb = np.vstack(X_buf)\n",
        "                yb = np.vstack(y_buf) if y_onehot else np.concatenate(y_buf).astype(np.int8)\n",
        "                for i in range(0, len(Xb), batch_size):\n",
        "                    yield Xb[i:i+batch_size], yb[i:i+batch_size]\n",
        "            gc.collect()\n",
        "\n",
        "def steps_per_epoch(total_rows, batch_size):\n",
        "    return math.ceil(total_rows / batch_size)\n",
        "\n",
        "# --------------------\n",
        "# Train NN (streamed over ALL rows)\n",
        "# --------------------\n",
        "print(\"[NN] Training…\")\n",
        "tf.keras.backend.clear_session()\n",
        "nn = build_model((feat_dim, 1), num_classes=2)\n",
        "loss_fn = get_loss(LOSS_CHOICE)\n",
        "lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(0.001, first_decay_steps=1000)\n",
        "nn.compile(optimizer=tf.keras.optimizers.Adam(lr_schedule), loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "def gen():\n",
        "    for xb, yb in stream_batches(FILES, BATCH_SIZE_NN, top_features, mean_dict, std_dict, y_onehot=True, repeat_epochs=EPOCHS):\n",
        "        yield (xb.reshape((xb.shape[0], xb.shape[1], 1)), yb)\n",
        "\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None, feat_dim, 1), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None, 2), dtype=tf.float32),\n",
        ")\n",
        "ds = tf.data.Dataset.from_generator(gen, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)\n",
        "spe = steps_per_epoch(total_rows, BATCH_SIZE_NN)\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)]\n",
        "nn.fit(ds, epochs=EPOCHS, steps_per_epoch=spe, verbose=2, class_weight=class_weight, callbacks=callbacks)\n",
        "\n",
        "# --------------------\n",
        "# Train XGBoost incrementally over ALL rows\n",
        "# --------------------\n",
        "print(\"[XGB] Training incrementally…\")\n",
        "xgb_params = {\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eval_metric\": \"logloss\",\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"max_depth\": XGB_MAX_DEPTH,\n",
        "    \"learning_rate\": 0.08,\n",
        "    \"subsample\": 0.8,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"max_bin\": 256,\n",
        "    \"reg_lambda\": 1.0,\n",
        "    \"nthread\": -1,\n",
        "    \"scale_pos_weight\": xgb_spw,\n",
        "    \"random_state\": RANDOM_STATE,\n",
        "}\n",
        "booster = None\n",
        "for xb, yb_1hot in stream_batches(FILES, batch_size=50_000, selected_cols=top_features,\n",
        "                                  means=mean_dict, stds=std_dict, y_onehot=True, repeat_epochs=1):\n",
        "    yflat = np.argmax(yb_1hot, axis=1).astype(np.float32).reshape(-1)\n",
        "    dtrain = xgb.DMatrix(xb, label=yflat)\n",
        "    booster = xgb.train(params=xgb_params, dtrain=dtrain,\n",
        "                        num_boost_round=XGB_ROUNDS_PER_BATCH, xgb_model=booster)\n",
        "    gc.collect()\n",
        "\n",
        "# --------------------\n",
        "# Evaluate (stream once over ALL rows)\n",
        "# --------------------\n",
        "print(\"[EVAL] Evaluating on ALL rows…\")\n",
        "y_true_all, y_pred_all = [], []\n",
        "cm = np.array([[0,0],[0,0]], dtype=np.int64)\n",
        "\n",
        "for xb, yb_1hot in stream_batches(FILES, batch_size=60_000, selected_cols=top_features,\n",
        "                                  means=mean_dict, stds=std_dict, y_onehot=True, repeat_epochs=1):\n",
        "    ytrue = np.argmax(yb_1hot, axis=1).astype(np.int64)\n",
        "\n",
        "    # NN probs\n",
        "    nn_probs = nn.predict(xb.reshape((xb.shape[0], xb.shape[1], 1)), verbose=0)\n",
        "    # XGB probs\n",
        "    dx = xgb.DMatrix(xb)\n",
        "    xgb_probs1 = booster.predict(dx)\n",
        "    xgb_probs  = np.stack([1.0 - xgb_probs1, xgb_probs1], axis=1)\n",
        "\n",
        "    # Ensemble\n",
        "    probs = ENSEMBLE_W_NN * nn_probs + ENSEMBLE_W_XGB * xgb_probs\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "    y_true_all.append(ytrue); y_pred_all.append(preds)\n",
        "    cm += confusion_matrix(ytrue, preds, labels=[0,1])\n",
        "    gc.collect()\n",
        "\n",
        "y_true_all = np.concatenate(y_true_all)\n",
        "y_pred_all = np.concatenate(y_pred_all)\n",
        "\n",
        "acc  = accuracy_score(y_true_all, y_pred_all)\n",
        "prec = precision_score(y_true_all, y_pred_all, average='weighted', zero_division=0)\n",
        "rec  = recall_score(y_true_all, y_pred_all, average='weighted', zero_division=0)\n",
        "f1   = f1_score(y_true_all, y_pred_all, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"\\n=== Metrics (ALL ROWS, merged days) ===\")\n",
        "print(classification_report(y_true_all, y_pred_all, digits=4))\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(\"Confusion Matrix [[TN, FP],[FN, TP]]:\\n\", cm)\n"
      ],
      "metadata": {
        "id": "CRDTu0CSpbhX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
